{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ0Tv8OmUQS0"
   },
   "source": [
    "# BentoML Demo: Iris Classifier with custom web UI\n",
    "\n",
    "\n",
    "**BentoML makes moving trained ML models to production easy:**\n",
    "\n",
    "* Package models trained with **any ML framework** and reproduce them for model serving in production\n",
    "* **Deploy anywhere** for online API serving or offline batch serving\n",
    "* High-Performance API model server with *adaptive micro-batching* support\n",
    "* Central hub for managing models and deployment process via Web UI and APIs\n",
    "* Modular and flexible design making it *adaptable to your infrastrcuture*\n",
    "\n",
    "BentoML is a framework for serving, managing, and deploying machine learning models. It is aiming to bridge the gap between Data Science and DevOps, and enable teams to deliver prediction services in a fast, repeatable, and scalable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vzf20abXU_vz"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5fANNcHQVC-g"
   },
   "outputs": [],
   "source": [
    "!pip install -q bentoml \"scikit-learn>=0.23.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a2_xA2JtCEo"
   },
   "source": [
    "## Create BentoService for model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ixGEZTxpVXPe",
    "outputId": "9c80ae20-f8e1-4933-8fe9-8e4498f23dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing iris_classifier.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of six failed: Traceback (most recent call last):\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 287, in update_class\n",
      "    old_obj = getattr(old, key)\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/six.py\", line 93, in __get__\n",
      "    setattr(obj, self.name, result)  # Invokes __set__.\n",
      "AttributeError: 'NoneType' object has no attribute 'cStringIO'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier.py\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "\n",
    "@env(infer_pip_packages=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "\n",
    "    @api(input=DataframeInput(), batch=True)\n",
    "    def predict(self, df):\n",
    "        # Optional pre-processing, post-processing code goes here\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LolMzwYrWaqI",
    "outputId": "cecd7fbe-7b5d-4f85-f94d-9db41fa19e6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load training data\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Model Training\n",
    "    clf = svm.SVC(gamma='scale')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Create a iris classifier service instance\n",
    "    iris_classifier_service = IrisClassifier()\n",
    "\n",
    "    # Pack the newly trained model artifact\n",
    "    iris_classifier_service.pack('model', clf)\n",
    "\n",
    "    # Save the prediction service to disk for model serving\n",
    "    saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0c5rH7IoL07F",
    "outputId": "06e2151f-09c8-401c-9ae5-855caf93201a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-12 03:58:43,153] INFO - BentoService bundle 'IrisClassifier:20210612035841_76D71A' saved to: /Users/lavi/bentoml/repository/IrisClassifier/20210612035841_76D71A\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mEDm1JZtZgp"
   },
   "source": [
    "## REST API Model Serving\n",
    "\n",
    "\n",
    "To start a REST API model server with the BentoService saved above, use the bentoml serve command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "PGo6Y_KNWjpY",
    "outputId": "3d6e17a0-64dd-490f-be7f-2cfe4fbf0d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-12 03:58:51,760] INFO - Getting latest version IrisClassifier:20210612035841_76D71A\n",
      "[2021-06-12 03:58:51,793] INFO - Starting BentoML API proxy in development mode..\n",
      "[2021-06-12 03:58:51,801] INFO - Starting BentoML API server in development mode..\n",
      "[2021-06-12 03:58:52,770] INFO - Your system nofile limit is 4096, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "======== Running on http://0.0.0.0:5000 ========\n",
      "(Press CTRL+C to quit)\n",
      " * Serving Flask app 'IrisClassifier' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:50922/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [12/Jun/2021 03:59:06] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 03:59:06] \"GET /static_content/main.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 03:59:06] \"GET /static_content/readme.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 03:59:06] \"GET /static_content/swagger-ui.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 03:59:06] \"GET /static_content/marked.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 03:59:06] \"GET /static_content/swagger-ui-bundle.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 03:59:06] \"GET /docs.json HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 03:59:06] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "[2021-06-12 03:59:44,812] ERROR - Error caught in API function:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/bentoml/service/inference_api.py\", line 177, in wrapped_func\n",
      "    return self._user_func(*args, **kwargs)\n",
      "  File \"/Users/lavi/bentoml/repository/IrisClassifier/20210612035841_76D71A/IrisClassifier/iris_classifier.py\", line 12, in predict\n",
      "    return self.artifacts.model.predict(df)\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/sklearn/svm/_base.py\", line 615, in predict\n",
      "    y = super().predict(X)\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/sklearn/svm/_base.py\", line 333, in predict\n",
      "    X = self._validate_for_predict(X)\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/sklearn/svm/_base.py\", line 466, in _validate_for_predict\n",
      "    order=\"C\", accept_large_sparse=False)\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/lavi/opt/anaconda3/envs/mlops/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 616, in check_array\n",
      "    \"if it contains a single sample.\".format(array))\n",
      "ValueError: Expected 2D array, got scalar array instead:\n",
      "array=nan.\n",
      "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
      "127.0.0.1 - - [12/Jun/2021 03:59:44] \"\u001b[31m\u001b[1mPOST /predict HTTP/1.1\u001b[0m\" 400 -\n",
      "[2021-06-12 04:00:05,508] INFO - {'service_name': 'IrisClassifier', 'service_version': '20210612035841_76D71A', 'api': 'predict', 'task': {'data': '[[1,2,1,2]]', 'task_id': '28d542e6-5698-401f-9d7f-e3c503aa7e73', 'batch': 1, 'http_headers': (('Host', '127.0.0.1:50922'), ('Connection', 'keep-alive'), ('Content-Length', '11'), ('Accept', '*/*'), ('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'), ('Content-Type', 'application/json'), ('Sec-Gpc', '1'), ('Origin', 'http://127.0.0.1:50922'), ('Sec-Fetch-Site', 'same-origin'), ('Sec-Fetch-Mode', 'cors'), ('Sec-Fetch-Dest', 'empty'), ('Referer', 'http://127.0.0.1:50922/'), ('Accept-Encoding', 'gzip, deflate, br'), ('Accept-Language', 'en-GB,en-US;q=0.9,en;q=0.8'))}, 'result': {'data': '[0]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': '28d542e6-5698-401f-9d7f-e3c503aa7e73'}\n",
      "127.0.0.1 - - [12/Jun/2021 04:00:05] \"POST /predict HTTP/1.1\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve IrisClassifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSds9kxuL07F"
   },
   "source": [
    "If you are running this notebook from Google Colab, you can start the dev server with `--run-with-ngrok` option, to gain acccess to the API endpoint via a public endpoint managed by [ngrok](https://ngrok.com/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34js5dtTL07F"
   },
   "outputs": [],
   "source": [
    "# !bentoml serveIrisiClassifier:latest --run-with-ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qllScrbHtqKl"
   },
   "source": [
    "At this point you can test out the Rest API server either by opening http://localhost:5000 in a new tab which will serve the swagger docs:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/bentoml/gallery/master/scikit-learn/iris-classifier/swagger.png)\n",
    "\n",
    "\n",
    "or by making a curl request in a another terminal window:\n",
    "\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[1,2,1,2]]' \\\n",
    "localhost:5000/predict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WN9nDgcvcul"
   },
   "source": [
    "## Adding Custom Web Static Content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "HO47OfDivier",
    "outputId": "1a9c658e-7422-4bbd-e75f-38860fec18ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  197k  100  197k    0     0   124k      0  0:00:01  0:00:01 --:--:--  124k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/bentoml/gallery/master/scikit-learn/iris-classifier/static.tar.xz -o static.tar.xz\n",
    "!tar --xz -xf static.tar.xz\n",
    "!rm static.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzJ1Wx-fyRcG"
   },
   "source": [
    "Here we have a very simple web ui as our static content that bento will serve.\n",
    "Now we will edit our bento service to point to this static directory.\n",
    "\n",
    "Add `@web_static_content('./static')` to `iris_classifier.py`\n",
    "\n",
    "**Note**: The path can be both relative or absolute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mgakbz1JxMpc",
    "outputId": "8f8ed48d-4686-430d-e215-1868ce76f0a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier.py\n",
    "from bentoml import env, artifacts, api, BentoService, web_static_content\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.artifact import SklearnModelArtifact\n",
    "\n",
    "@env(auto_pip_dependencies=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "@web_static_content('./static')\n",
    "class IrisClassifier(BentoService):\n",
    "\n",
    "    @api(input=DataframeInput(), batch=True)\n",
    "    def test(self, df):\n",
    "        # Optional pre-processing, post-processing code goes here\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "irZjCcy1L07G",
    "outputId": "244a21ab-6e83-4ebb-a8e7-6f65d9318669",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-12 04:00:59,826] WARNING - Importing from \"bentoml.artifact.*\" has been deprecated. Instead, use`bentoml.frameworks.*` and `bentoml.service.*`. e.g.:, `from bentoml.frameworks.sklearn import SklearnModelArtifact`, `from bentoml.service.artifacts import BentoServiceArtifact`, `from bentoml.service.artifacts.common import PickleArtifact`\n",
      "[2021-06-12 04:00:59,830] WARNING - `auto_pip_dependencies` parameter in `@env` is being deprecated soon,use `infer_pip_packages` instead, e.g. `@env(infer_pip_packages=True)`\n",
      "[2021-06-12 04:01:01,504] INFO - BentoService bundle 'IrisClassifier:20210612040100_AC97D6' saved to: /Users/lavi/bentoml/repository/IrisClassifier/20210612040100_AC97D6\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "rfbaTgKW0D78",
    "outputId": "558b2660-2a45-4d4e-db66-9a559ace69a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-12 04:01:06,357] INFO - Getting latest version IrisClassifier:20210612040100_AC97D6\n",
      "[2021-06-12 04:01:06,390] INFO - Starting BentoML API proxy in development mode..\n",
      "[2021-06-12 04:01:06,394] INFO - Starting BentoML API server in development mode..\n",
      "[2021-06-12 04:01:06,520] INFO - Your system nofile limit is 4096, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "======== Running on http://0.0.0.0:5000 ========\n",
      "(Press CTRL+C to quit)\n",
      "[2021-06-12 04:01:07,751] WARNING - Importing from \"bentoml.artifact.*\" has been deprecated. Instead, use`bentoml.frameworks.*` and `bentoml.service.*`. e.g.:, `from bentoml.frameworks.sklearn import SklearnModelArtifact`, `from bentoml.service.artifacts import BentoServiceArtifact`, `from bentoml.service.artifacts.common import PickleArtifact`\n",
      "[2021-06-12 04:01:07,764] WARNING - `auto_pip_dependencies` parameter in `@env` is being deprecated soon,use `infer_pip_packages` instead, e.g. `@env(infer_pip_packages=True)`\n",
      " * Serving Flask app 'IrisClassifier' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:50966/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [12/Jun/2021 04:01:11] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 04:01:11] \"GET /css/styles.css?v=1.0 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 04:01:11] \"GET /js/scripts.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 04:01:11] \"GET /img/setosa.jpeg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 04:01:11] \"GET /img/versicolor.jpeg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 04:01:11] \"GET /img/verginica.jpeg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2021 04:01:12] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "[2021-06-12 04:01:17,621] INFO - {'service_name': 'IrisClassifier', 'service_version': '20210612040100_AC97D6', 'api': 'test', 'task': {'data': '[[\"1.23\",\"3.07\",\"2.25\",\"0.45\"]]', 'task_id': '055f8275-c3b9-43bc-a34e-bea41f35b2cd', 'batch': 1, 'http_headers': (('Host', '127.0.0.1:50966'), ('Connection', 'keep-alive'), ('Content-Length', '31'), ('Access-Control-Allow-Origin', '*'), ('Accept', '*/*'), ('Accept-Language', 'en-US,en;q=0.5'), ('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'), ('Content-Type', 'application/json'), ('Sec-Gpc', '1'), ('Origin', 'http://127.0.0.1:50966'), ('Sec-Fetch-Site', 'same-origin'), ('Sec-Fetch-Mode', 'cors'), ('Sec-Fetch-Dest', 'empty'), ('Referer', 'http://127.0.0.1:50966/'), ('Accept-Encoding', 'gzip, deflate, br'))}, 'result': {'data': '[0]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': '055f8275-c3b9-43bc-a34e-bea41f35b2cd'}\n",
      "127.0.0.1 - - [12/Jun/2021 04:01:17] \"POST /test HTTP/1.1\" 200 -\n",
      "[2021-06-12 04:01:21,866] INFO - {'service_name': 'IrisClassifier', 'service_version': '20210612040100_AC97D6', 'api': 'test', 'task': {'data': '[[\"3.86\",\"4.83\",\"3.21\",\"2.00\"]]', 'task_id': '18f9d938-6644-4714-9174-2f1c3f731e2e', 'batch': 1, 'http_headers': (('Host', '127.0.0.1:50966'), ('Connection', 'keep-alive'), ('Content-Length', '31'), ('Access-Control-Allow-Origin', '*'), ('Accept', '*/*'), ('Accept-Language', 'en-US,en;q=0.5'), ('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'), ('Content-Type', 'application/json'), ('Sec-Gpc', '1'), ('Origin', 'http://127.0.0.1:50966'), ('Sec-Fetch-Site', 'same-origin'), ('Sec-Fetch-Mode', 'cors'), ('Sec-Fetch-Dest', 'empty'), ('Referer', 'http://127.0.0.1:50966/'), ('Accept-Encoding', 'gzip, deflate, br'))}, 'result': {'data': '[0]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': '18f9d938-6644-4714-9174-2f1c3f731e2e'}\n",
      "127.0.0.1 - - [12/Jun/2021 04:01:21] \"POST /test HTTP/1.1\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve IrisClassifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIOGBpOZ0GoL"
   },
   "source": [
    "Now if you visit http://localhost:5000/, you should be served with a beautiful UI:\n",
    "\n",
    "![Custom UI](https://raw.githubusercontent.com/bentoml/gallery/master/scikit-learn/iris-classifier/webui.png)\n",
    "\n",
    "It's still possible to access the swagger docs at `/docs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwXAgmPcL07H"
   },
   "source": [
    "## Containerize model server with Docker\n",
    "\n",
    "\n",
    "One common way of distributing this model API server for production deployment, is via Docker containers. And BentoML provides a convenient way to do that.\n",
    "\n",
    "Note that docker is **not available in Google Colab**. You will need to download and run this notebook locally to try out this containerization with docker feature.\n",
    "\n",
    "If you already have docker configured, simply run the follow command to product a docker container serving the IrisClassifier prediction service created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e8Qm0q3jL07H",
    "outputId": "7ce609be-f733-4c9b-a7ac-62b03eb5ebf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-12 04:03:25,403] INFO - Getting latest version IrisClassifier:20210612040100_AC97D6\n",
      "\u001b[39mFound Bento: /Users/lavi/bentoml/repository/IrisClassifier/20210612040100_AC97D6\u001b[0m\n",
      "Containerizing IrisClassifier:20210612040100_AC97D6 with local YataiService and docker daemon from local environment\\\u001b[32mBuild container image: irisclassifier:20210612040100_AC97D6\u001b[0m\n",
      "\b \r"
     ]
    }
   ],
   "source": [
    "!bentoml containerize IrisClassifier:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "q_67Aj6SL07H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      "[2021-06-11 22:35:54,530] INFO - Starting BentoML proxy in production mode..\n",
      "[2021-06-11 22:35:54,535] INFO - Starting BentoML API server in production mode..\n",
      "[2021-06-11 22:35:54 +0000] [20] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-06-11 22:35:55 +0000] [20] [INFO] Listening at: http://0.0.0.0:32855 (20)\n",
      "[2021-06-11 22:35:55 +0000] [20] [INFO] Using worker: sync\n",
      "[2021-06-11 22:35:55 +0000] [22] [INFO] Booting worker with pid: 22\n",
      "[2021-06-11 22:35:55,010] INFO - Running micro batch service on :5000\n",
      "[2021-06-11 22:35:55 +0000] [1] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-06-11 22:35:55 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n",
      "[2021-06-11 22:35:55 +0000] [1] [INFO] Using worker: aiohttp.worker.GunicornWebWorker\n",
      "[2021-06-11 22:35:55 +0000] [24] [INFO] Booting worker with pid: 24\n",
      "[2021-06-11 22:35:55,155] WARNING - Saved BentoService Python version mismatch: loading BentoService bundle created with Python version 3.7.10, but current environment version is 3.7.6.\n",
      "[2021-06-11 22:35:55,167] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "[2021-06-11 22:35:56,623] WARNING - Saved BentoService Python version mismatch: loading BentoService bundle created with Python version 3.7.10, but current environment version is 3.7.6.\n",
      "[2021-06-11 22:35:56,626] WARNING - Importing from \"bentoml.artifact.*\" has been deprecated. Instead, use`bentoml.frameworks.*` and `bentoml.service.*`. e.g.:, `from bentoml.frameworks.sklearn import SklearnModelArtifact`, `from bentoml.service.artifacts import BentoServiceArtifact`, `from bentoml.service.artifacts.common import PickleArtifact`\n",
      "[2021-06-11 22:35:56,644] WARNING - `auto_pip_dependencies` parameter in `@env` is being deprecated soon,use `infer_pip_packages` instead, e.g. `@env(infer_pip_packages=True)`\n",
      "^C\n",
      "[2021-06-11 22:40:11 +0000] [1] [INFO] Handling signal: int\n",
      "[2021-06-11 22:40:11 +0000] [24] [INFO] Worker exiting (pid: 24)\n",
      "[2021-06-11 22:40:11 +0000] [24] [WARNING] Exception during worker exit:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/gunicorn/arbiter.py\", line 589, in spawn_worker\n",
      "    worker.init_process()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/aiohttp/worker.py\", line 51, in init_process\n",
      "    super().init_process()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/gunicorn/workers/base.py\", line 142, in init_process\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/aiohttp/worker.py\", line 64, in run\n",
      "    sys.exit(self.exit_code)\n",
      "SystemExit: 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/gunicorn/arbiter.py\", line 608, in spawn_worker\n",
      "    self.cfg.worker_exit(self, worker)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/bentoml/server/gunicorn_config.py\", line 8, in worker_exit\n",
      "    multiprocess.mark_process_dead(worker.pid)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/prometheus_client/multiprocess.py\", line 161, in mark_process_dead\n",
      "    for f in glob.glob(os.path.join(path, 'gauge_livesum_{0}.db'.format(pid))):\n",
      "  File \"/opt/conda/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "\n",
      "[2021-06-11 22:40:11 +0000] [20] [INFO] Handling signal: term\n",
      "[2021-06-11 22:40:11 +0000] [22] [INFO] Worker exiting (pid: 22)\n",
      "[2021-06-11 22:40:11 +0000] [22] [WARNING] Exception during worker exit:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/gunicorn/arbiter.py\", line 590, in spawn_worker\n",
      "    sys.exit(0)\n",
      "SystemExit: 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/gunicorn/arbiter.py\", line 608, in spawn_worker\n",
      "    self.cfg.worker_exit(self, worker)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/bentoml/server/gunicorn_config.py\", line 8, in worker_exit\n",
      "    multiprocess.mark_process_dead(worker.pid)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/prometheus_client/multiprocess.py\", line 161, in mark_process_dead\n",
      "    for f in glob.glob(os.path.join(path, 'gauge_livesum_{0}.db'.format(pid))):\n",
      "  File \"/opt/conda/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm -p 5000:5000 irisclassifier:20210612040100_AC97D6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWhMwMtpL07H"
   },
   "source": [
    "## Launch inference job from CLI\n",
    "\n",
    "BentoML cli supports loading and running a packaged model from CLI. With the DataframeInput adapter, the CLI command supports reading input Dataframe data from CLI argument or local csv or json files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uwPMMd06L07H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-12 04:10:17,947] INFO - Getting latest version IrisClassifier:20210612040100_AC97D6\n",
      "[2021-06-12 04:10:19,373] WARNING - Importing from \"bentoml.artifact.*\" has been deprecated. Instead, use`bentoml.frameworks.*` and `bentoml.service.*`. e.g.:, `from bentoml.frameworks.sklearn import SklearnModelArtifact`, `from bentoml.service.artifacts import BentoServiceArtifact`, `from bentoml.service.artifacts.common import PickleArtifact`\n",
      "[2021-06-12 04:10:19,382] WARNING - `auto_pip_dependencies` parameter in `@env` is being deprecated soon,use `infer_pip_packages` instead, e.g. `@env(infer_pip_packages=True)`\n",
      "Error: \u001b[31mbentoml-cli run failed: Can't find API 'predict' in service 'IrisClassifier'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bentoml run IrisClassifier:latest predict --input '[[5, 4, 3, 2]]'  # doesn't run on M1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvzMpaSrL07H"
   },
   "source": [
    "## Load saved BentoService\n",
    "\n",
    "bentoml.load is the API for loading a BentoML packaged model in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "P16XRxzXL07H"
   },
   "outputs": [],
   "source": [
    "from bentoml import load\n",
    "\n",
    "service = load(saved_path)\n",
    "\n",
    "print(service.predict([[5,4,3,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kn2ZIU_jL07H"
   },
   "source": [
    "# Deployment Options\n",
    "\n",
    "If you are at a small team with limited engineering or DevOps resources, try out automated deployment with BentoML CLI, currently supporting AWS Lambda, AWS SageMaker, and Azure Functions:\n",
    "- [AWS Lambda Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_lambda.html)\n",
    "- [AWS SageMaker Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html)\n",
    "- [Azure Functions Deployment Guide](https://docs.bentoml.org/en/latest/deployment/azure_functions.html)\n",
    "\n",
    "If the cloud platform you are working with is not on the list above, try out these step-by-step guide on manually deploying BentoML packaged model to cloud platforms:\n",
    "- [AWS ECS Deployment](https://docs.bentoml.org/en/latest/deployment/aws_ecs.html)\n",
    "- [Google Cloud Run Deployment](https://docs.bentoml.org/en/latest/deployment/google_cloud_run.html)\n",
    "- [Azure container instance Deployment](https://docs.bentoml.org/en/latest/deployment/azure_container_instance.html)\n",
    "- [Heroku Deployment](https://docs.bentoml.org/en/latest/deployment/heroku.html)\n",
    "\n",
    "Lastly, if you have a DevOps or ML Engineering team who's operating a Kubernetes or OpenShift cluster, use the following guides as references for implementating your deployment strategy:\n",
    "- [Kubernetes Deployment](https://docs.bentoml.org/en/latest/deployment/kubernetes.html)\n",
    "- [Knative Deployment](https://docs.bentoml.org/en/latest/deployment/knative.html)\n",
    "- [Kubeflow Deployment](https://docs.bentoml.org/en/latest/deployment/kubeflow.html)\n",
    "- [KFServing Deployment](https://docs.bentoml.org/en/latest/deployment/kfserving.html)\n",
    "- [Clipper.ai Deployment Guide](https://docs.bentoml.org/en/latest/deployment/clipper.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kqge2YWxL07I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BentoML.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
